{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - ZOMATO RESTURANT CLUSTERING AND SENTIMENT ANALYSIS\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised Machine Learning\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member 1 -**   Sonali Singh\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Zomato is an Indian restaurant aggregator and food delivery start-up founded by Deepinder Goyal and Pankaj Chaddah in 2008. Zomato provides information, menus and user-reviews of restaurants, and also has food delivery options from partner restaurants in select cities.\n",
        "\n",
        "India is quite famous for its diverse multi cuisine available in a large number of restaurants and hotel resorts, which is reminiscent of unity in diversity. Restaurant business in India is always evolving. More Indians are warming up to the idea of eating restaurant food whether by dining outside or getting food delivered. The growing number of restaurants in every state of India has been a motivation to inspect the data to get some insights, interesting facts and figures about the Indian food industry in each city. So, this project focuses on analysing the Zomato restaurant data for each city in India.\n",
        "\n",
        "There are two separate files, while the columns are self explanatory. Below is a brief description:\n",
        "\n",
        "Restaurant names and Metadata - This could help in clustering the restaurants into segments. Also the data has valuable information around cuisine and costing which can be used in cost vs. benefit analysis Restaurant reviews - Data could be used for sentiment analysis. Also the metadata of reviewers can be used for identifying the critics in the industry.\n",
        "\n",
        "Steps that are performed:\n",
        "\n",
        "* Importing libraries\n",
        "\n",
        "* Loading the dataset\n",
        "\n",
        "* Shape of dataset\n",
        "\n",
        "* Dataset information\n",
        "\n",
        "* Handling the duplicate values\n",
        "\n",
        "* Handling missing values.\n",
        "\n",
        "* Undeerstanding the columns\n",
        "\n",
        "* Variable description\n",
        "\n",
        "* Data wrangling\n",
        "\n",
        "* Data visualization\n",
        "\n",
        "* Story telling and experimenting with charts.\n",
        "\n",
        "* Text preprocessing,\n",
        "\n",
        "* Latent Direchlet Allocation\n",
        "\n",
        "* Sentiment analysis\n",
        "\n",
        "* Challenges faced\n",
        "\n",
        "* Conclusion."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem statement for this project is to analyze and understand the restaurant industry in India by utilizing data from the Indian restaurant aggregator and food delivery start-up, Zomato. The project aims to gain insights into the sentiments of customer reviews, cluster Zomato restaurants into different segments, and analyze the data to make useful conclusions in the form of visualizations. The data analyzed includes information on cuisine, costing, and customer reviews. The project aims to assist customers in finding the best restaurant in their locality and aid the company in identifying areas for growth and improvement in the industry. Additionally, the project aims to use the data for sentiment analysis and identifying critics in the industry through the metadata of reviewers."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from textblob import TextBlob\n",
        "from IPython.display import Image\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.utils import simple_preprocess\n",
        "import gensim\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# importing dataset\n",
        "meta_df = pd.read_csv('/content/drive/MyDrive/MACHINELEARNING/Zomato Restaurant names and Metadata (2) - Copy.csv')\n",
        "review_df = pd.read_csv('/content/drive/MyDrive/MACHINELEARNING/Zomato Restaurant reviews (2).csv')"
      ],
      "metadata": {
        "id": "he2zZcLsiDMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "meta_df=meta_df_main"
      ],
      "metadata": {
        "id": "mIukEgWuEQTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "meta_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dataset First Look review\n",
        "review_df.head()"
      ],
      "metadata": {
        "id": "-TqEmvXur3F3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dataset Rows & Columns count.\n",
        "\n",
        "print(f' We have total {meta_df.shape[0]} rows and {meta_df.shape[1]} columns.')\n",
        "\n",
        "print(f' We have total {review_df.shape[0]} rows and {review_df.shape[1]} columns.')"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IN4GXFlbsshB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "meta_df.info()\n",
        "review_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dataset Duplicate Value Count.\n",
        "\n",
        "meta_df.duplicated(keep='last').sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_df.duplicated(keep='last').sum()"
      ],
      "metadata": {
        "id": "IiC3C4EJuD05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "\n",
        "meta_df.isnull().sum()\n"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_df.isnull().sum()"
      ],
      "metadata": {
        "id": "8MX4HuA9sBTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for Null values.\n",
        "\n",
        "meta_df[meta_df['Collections'].isnull()].head()"
      ],
      "metadata": {
        "id": "qmpxTGUNsPNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_df[review_df['Metadata'].isnull()].head()"
      ],
      "metadata": {
        "id": "mNFfyTqtsYTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(15,5))\n",
        "sns.heatmap(meta_df.isnull(),cmap='plasma',annot=False,yticklabels=False)\n",
        "plt.title(\" Visualising Missing Values\");"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualizing the missing values for reviews\n",
        "# Checking Null Value by plotting Heatmap\n",
        "sns.heatmap(review_df.isnull(), cbar=False);"
      ],
      "metadata": {
        "id": "7S3AS_JDudDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Restaurant DataSet**\n",
        "\n",
        "* There are 105 total observation with 6 different features.\n",
        "\n",
        "* Feature like collection and timing has null values.\n",
        "\n",
        "* There is no duplicate values i.e., 105 unique data.\n",
        "\n",
        "* Feature cost represent amount but has object data type because these values are separated by comma ','.\n",
        "\n",
        "* Timing represent operational hour but as it is represented in the form of text has object data type."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Review DataSet**\n",
        "\n",
        "* There are total 10000 observation and 7 features.\n",
        "\n",
        "* Except picture and restaurant feature all others have null values.\n",
        "\n",
        "* There are total of 36 duplicate values for two restaurant - American Wild Wings and Arena Eleven, where all these duplicate values generally have null values.\n",
        "\n",
        "* Rating represent ordinal data, has object data type should be integer.\n",
        "\n",
        "* Timing represent the time when review was posted but show object data time, it should be converted into date time."
      ],
      "metadata": {
        "id": "dwfX-jRwvvrk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "meta_df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_df.columns"
      ],
      "metadata": {
        "id": "BhSWMK9pw9ES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "meta_df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_df.describe(include='all')"
      ],
      "metadata": {
        "id": "DTJ1a0eJxmwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zomato Restaurant names and Metadata**\n",
        "\n",
        "1. Name : Name of Restaurants\n",
        "\n",
        "2. Links : URL Links of Restaurants\n",
        "\n",
        "3. Cost : Per person estimated Cost of dining\n",
        "\n",
        "4. Collection : Tagging of Restaurants w.r.t. Zomato categories\n",
        "\n",
        "5. Cuisines : Cuisines served by Restaurants\n",
        "\n",
        "6. Timings : Restaurant Timings"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Zomato Restaurant reviews**\n",
        "\n",
        "1. Restaurant : Name of the Restaurant\n",
        "\n",
        "2. Reviewer : Name of the Reviewer\n",
        "\n",
        "3. Review : Review Text\n",
        "\n",
        "4. Rating : Rating Provided by Reviewer\n",
        "\n",
        "5. MetaData : Reviewer Metadata - No. of Reviews and followers\n",
        "\n",
        "6. Time: Date and Time of Review\n",
        "\n",
        "7. Pictures : No. of pictures posted with review"
      ],
      "metadata": {
        "id": "Gt68oKd0zGUN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "# Check Unique Values for each variable for restaurant\n",
        "for i in meta_df.columns.tolist():\n",
        "  print(\"No. of unique values in \",i,\"is\",meta_df[i].nunique(),\".\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Check Unique Values for each variable for reviews\n",
        "for i in review_df.columns.tolist():\n",
        "  print(\"No. of unique values in \",i,\"is\",review_df[i].nunique(),\".\")"
      ],
      "metadata": {
        "id": "A07qmlUd0BCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Convert the 'Cost' column, deleting the comma and changing the data type into 'int64'.\n",
        "\n",
        "meta_df['Cost'] =  meta_df['Cost'].str.replace(\",\",\"\").astype('int64')\n"
      ],
      "metadata": {
        "id": "8yIZHsqQh5qE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the 'Cost' column, deleting the comma and changing the data type into 'int64'"
      ],
      "metadata": {
        "id": "GsCK7lNoO5D2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info.\n",
        "\n",
        "meta_df.info()"
      ],
      "metadata": {
        "id": "OKGc_8Q8O-Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review_df.info()"
      ],
      "metadata": {
        "id": "VhhkegptPewv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Firstly, I started with changing data types for cost and rating. In rating there was only one rating which was string or has value of like so I change it into median of the rating. This was done to make data consistent.\n",
        "\n",
        "Restaurant data : In this dataset I first figured out 5 costlier restaurant in which Collage - Hyatt Hyderabad Gachibowli has maximum price of 2800 and then found the lowest which is Amul with price of 150. Then I found how many hotel share same price i.e., 13 hotel share 500 price. North indian cuisine with great buffet tags is mostly used in hotels.\n",
        "\n",
        "Review data : In this dataset I found famous or restaurant that show maximum engagement. Followed by that I found most followed critic which was Satwinder Singh who posted total of 186 reviews and had followers of 13410 who gives and average of 3.67 rating for each order he makes. Lastly I also found in year 2018 4903 hotels got reviews.\n",
        "\n",
        "Then I merged the two dataset together to figure out the price point for the restaurant, top rated restaurant AB's - Absolute Barbecues has a price point of 1500 and the low rated Hotel Zara Hi-Fi has price point of 400.\n",
        "\n",
        "In order to exactly understand why even with price point of 1500 these hotel has maximum number of rating and sentiment of those rating need to extract words from the text and do futher analysis of the review and then followed by forming clusters so that one can get recommendation about top quality restaurants."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "\n",
        "top10_res_by_cost = meta_df[['Name','Cost']].groupby('Name',as_index=False).sum().sort_values(by='Cost',ascending=False).head(10)"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating word cloud for expensive restaurants.\n",
        "plt.figure(figsize=(15,8))\n",
        "text = \" \".join(name for name in meta_df.sort_values('Cost',ascending=False).Name[:30])\n",
        "\n",
        "# Creating word_cloud with text as argument in .generate() method.\n",
        "word_cloud = WordCloud(width = 1400, height = 1400,collocations = False, background_color = 'black').generate(text)\n",
        "\n",
        "# Display the generated Word Cloud.\n",
        "plt.imshow(word_cloud, interpolation='bilinear')\n",
        "\n",
        "plt.axis(\"off\");"
      ],
      "metadata": {
        "id": "SkxBzVl4UuQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pie chart provides a clear and simple way to see the proportion of different food attributes, making it easy to identify the most popular attributes and compare them to one another. It also allows for a quick comparison of the popularity of different attributes, and can be useful in identifying patterns or trends in the data.\n",
        "\n",
        "On the other hand, a word cloud displays the most frequently mentioned attributes in a way that is visually striking and easy to understand. It is useful for identifying the most frequently mentioned attributes and can be used to quickly identify patterns and trends in customer reviews.\n",
        "\n",
        "Both charts, when used together, can provide a comprehensive understanding of customer reviews and can be used to identify customer preferences, which can help Zomato to make strategic decisions to improve their business"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Marriott is the most frequently used tags and other tags like Hotel,Holiday,Sheraton,Bar, Hyderabad is also used in large quantity."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting a pie chart of tags used to describe food can help a restaurant review and food delivery platform Zomato to identify the most popular adjectives used to describe the food. This information can be used to make strategic decisions about which food attributes to focus on promoting and expanding. For example, if a significant portion of customers are describing the food as \"delicious\" or \"fresh\", Zomato could focus on adding more restaurants that are known for their delicious and fresh food and promoting them to customers.\n",
        "\n",
        "Similarly, a word cloud of tags used to describe food can help Zomato identify the most frequently mentioned food attributes in customer reviews. This can provide insight into which attributes are most popular and well-regarded among customers, and which attributes may need improvement.\n",
        "\n",
        "However, it's important to note that these types of charts do not provide all the information about the business, and can not be the only decision making factor. For example, a pie chart showing that a certain adjective is popular does not tell us about the profitability of that adjective or the competition in that category. The same goes for word cloud, it only shows us the frequency of the adjective mentioned, it can not tell us if the mentions are positive or negative.\n",
        "\n",
        "Additionally, these charts do not provide information about the other factors that can impact the business such as market trends, consumer preferences, and economic conditions. Therefore, it's important for Zomato to consider other data and information when making strategic decisions. Also, it's important to note that the data used for creating these charts should be cleaned and validated, as the results may be biased if the data is not accurate or complete."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "plt.figure(figsize=(15,6))\n",
        "\n",
        "# Performing groupby To get values accourding to Names and sort it for visualisation.\n",
        "top_10_affor_rest=meta_df[['Name','Cost']].groupby('Name',as_index=False).sum().sort_values(by='Cost',ascending=False).tail(10)\n",
        "\n",
        "# Lables for X and Y axis\n",
        "x = top_10_affor_rest['Cost']\n",
        "y = top_10_affor_rest['Name']\n",
        "\n",
        "# Assigning the arguments for chart\n",
        "plt.title(\"Top 10 Affordable Restaurant\",fontsize=20, weight='bold',color=sns.cubehelix_palette(8, start=.5, rot=-.75)[-3])\n",
        "plt.ylabel(\"Name\",weight='bold',fontsize=15)\n",
        "plt.xlabel(\"Cost\",weight='bold',fontsize=15)\n",
        "plt.xticks(rotation=90)\n",
        "sns.barplot(x=x, y=y,palette='rocket')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Barplot helps in plotting the frquency of cost for each restaurant."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above chart it is clear that restaurant Collage - The Old Madras Banking Company is most expensive restaurant in the locality which has a price of 350 for order and has 3.5 average rating.\n",
        "\n",
        "Hotels like Amul and Mohammedia Shawarma are least expensive with price of 150 and has 3.9 average rating"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most expensive product are always center of attraction for a niche market (subset of the market on which a specific product is focused) at the same time for a business purpose, this product are preffered to be most revenue generating market.\n",
        "\n",
        "Definetly for food delivery platform Zomato, it is very important to focus and improve sales based on these hotels.\n",
        "\n",
        "Based on the average rating of 3.4 these product should increase their engagement as this may cause negative brand impact. However true behaviour can only be inspected through analysing of reviews."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Visualisation the value counts of collection.\n",
        "meta_df['Collections'].value_counts()[0:10].sort_values().plot(figsize=(10,8),kind='barh')\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Barplot helps in plotting the frquency of cost for each restaurant."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting bar chart shows the top 10 most frequent values in the Collections column on the y-axis and their corresponding counts on the x-axis. The horizontal orientation of the bars makes it easy to compare the counts of the different collections. The longer the bar, the higher the count."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##Text preprocessing for the meta dataset.\n",
        "\n",
        "In Order to plot the cuisines from the data we have to count the frequency of the words from the document.(Frequency of cuisine). For that We have to perform the opration like removing stop words, Convert all the text into lower case, removing punctuations, removing repeated charactors, removing Numbers and emojies and finally count vectorizer."
      ],
      "metadata": {
        "id": "t2td3JP3n7DT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading and importing the dependancies for text cleaning.\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "-U1WdUw5oQyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the stopwords from nltk library for English corpus.\n",
        "sw = stopwords.words('english')"
      ],
      "metadata": {
        "id": "jhPNXDPzogjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a function for removing stopwords.\n",
        "def stopwords(text):\n",
        "    '''a function for removing the stopword'''\n",
        "\n",
        "    # removing the stop words and lowercasing the selected words\n",
        "    text = [word.lower() for word in str(text).split() if word.lower() not in sw]\n",
        "\n",
        "    # joining the list of words with space separator\n",
        "    return \" \".join(text)"
      ],
      "metadata": {
        "id": "DtQ_XkJnojlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing stopwords from Cuisines.\n",
        "meta_df['Cuisines'] = meta_df['Cuisines'].apply(lambda text: stopwords(text))\n",
        "meta_df['Cuisines'].head()"
      ],
      "metadata": {
        "id": "KxYf5hwbop9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Punctuations present in the text are removed successfully"
      ],
      "metadata": {
        "id": "FNBVcmfhpceW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaning and removing Numbers.\n",
        "import re\n",
        "\n",
        "# Writing a function to remove repeating characters.\n",
        "def cleaning_repeating_char(text):\n",
        "    return re.sub(r'(.)1+', r'1', text)"
      ],
      "metadata": {
        "id": "da1PEOdCpe5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Removing repeating characters from Cuisines.\n",
        "meta_df['Cuisines'] = meta_df['Cuisines'].apply(lambda x: cleaning_repeating_char(x))\n",
        "meta_df['Cuisines'].head()\n"
      ],
      "metadata": {
        "id": "sNn6R71cpl4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removed repeated characters successfully"
      ],
      "metadata": {
        "id": "DMoapEcfro89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Removing the Numbers from the data.\n",
        "def cleaning_numbers(data):\n",
        "    return re.sub('[0-9]+', '', data)"
      ],
      "metadata": {
        "id": "FfH3FWx8r4d3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Implementing the cleaning.\n",
        "meta_df['Cuisines'] = meta_df['Cuisines'].apply(lambda x: cleaning_numbers(x))\n",
        "meta_df['Cuisines'].head()\n"
      ],
      "metadata": {
        "id": "Wn0dxUABsNLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We dont want numbers in the text Hence removed number successfully"
      ],
      "metadata": {
        "id": "dZ0ZiXxOsbke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Top 20 Two word Frequencies of Cuisines.\n",
        "from collections import Counter\n",
        "text = ' '.join(meta_df['Cuisines'])\n",
        "\n",
        "# separating each word from the sentences\n",
        "words = text.split()\n",
        "\n",
        "# Extracting the first word from the number for cuisines in the sentence.\n",
        "two_words = {' '.join(words):n for words,n in Counter(zip(words, words[1:])).items() if not  words[0][-1]==(',')}"
      ],
      "metadata": {
        "id": "qE9ZUH3asdSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Extracting the most frequent cuisine present in the collection.\n",
        "# Counting a frequency for cuisines.\n",
        "two_words_dfc = pd.DataFrame(two_words.items(), columns=['Cuisine Words', 'Frequency'])\n",
        "\n",
        "# Sorting the most frequent cuisine at the top and order by descending\n",
        "two_words_dfc = two_words_dfc.sort_values(by = \"Frequency\", ascending = False)\n",
        "\n",
        "# selecting first top 20 frequent cuisine.\n",
        "two_words_20c = two_words_dfc[:20]\n",
        "two_words_20c\n"
      ],
      "metadata": {
        "id": "8ege_EqMsogi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Visualizing the frequency of the Cuisines.\n",
        "\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.figure(figsize = (18, 8))\n",
        "sns.barplot(y = \"Cuisine Words\", x = \"Frequency\", data = two_words_20c, palette = \"magma\")\n",
        "plt.title(\"Top 20 Two-word Frequencies of Cuisines\", size = 20)\n",
        "plt.xticks(size = 15)\n",
        "plt.yticks(size = 15)\n",
        "plt.xlabel(\"Cuisine Words\", size = 20)\n",
        "plt.ylabel(None)\n",
        "plt.savefig(\"Top_20_Two-word_Frequencies_of_Cuisines.png\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The DataFrame contains two columns: \"Cuisine Words\" and \"Frequency.\" The \"Cuisine Words\" column lists the most frequent two-word cuisine terms, while the \"Frequency\" column shows the number of times each two-word cuisine term appears in the dataset.This information can be helpful in understanding the most common cuisine types in the dataset. It can also be used to identify trends and patterns in the types of cuisines that are popular or in demand among the customers."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# proportion or percentage of occurrences for each unique value in the Rating column.\n",
        "review_df['Rating'].value_counts(normalize=True)\n"
      ],
      "metadata": {
        "id": "VHF2JCL17wRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Removing like value and taking the mean in the rating column.\n",
        "review_df.loc[review_df['Rating'] == 'Like'] = np.nan\n",
        "\n",
        " # Chenging the data type of rating column\n",
        "review_df['Rating']= review_df['Rating'].astype('float64')\n",
        "\n",
        "print(review_df['Rating'].mean())"
      ],
      "metadata": {
        "id": "nyKAsV2F75HB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Filling mean in place of null value\n",
        "review_df['Rating'].fillna(3.6, inplace=True)"
      ],
      "metadata": {
        "id": "V0Wo1Y5c7-Ju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Changing the data type of review column.\n",
        "review_df['Review'] = review_df['Review'].astype(str)\n",
        "\n",
        "# Creating a review_length column to check the frequency of each rating.\n",
        "review_df['Review_length'] = review_df['Review'].apply(len)\n"
      ],
      "metadata": {
        "id": "6AS5dfar8Dox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "review_df['Rating'].value_counts(normalize=True)"
      ],
      "metadata": {
        "id": "hbrGMfuh8JNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Ratings distribution 38% reviews are 5 rated,23% are 4 rated stating that people do rate good food high."
      ],
      "metadata": {
        "id": "ZhlF5ho18XV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Visualizing the rating column against the review length.\n",
        "# Polting the frequency of the rating on scatter bar plot\n",
        "\n",
        "import plotly.express as px\n",
        "fig = px.scatter(review_df, x=review_df['Rating'], y=review_df['Review_length'])\n",
        "fig.update_layout(title_text=\"Rating vs Review Length\")\n",
        "fig.update_xaxes(ticks=\"outside\", tickwidth=1, tickcolor='crimson',tickangle=45, ticklen=10)\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scatter plot confirms that length of review doesnt impact ratings."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "\n",
        "# Creating polarity variable to see sentiments in reviews.(using textblob)\n",
        "from textblob import TextBlob\n",
        "review_df['Polarity'] = review_df['Review'].apply(lambda x: TextBlob(x).sentiment.polarity)\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualizing the polarity using histogram.\n",
        "review_df['Polarity'].plot(kind='hist', bins=100)"
      ],
      "metadata": {
        "id": "t5TXRg1K9hrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polarity is float which lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement. Subjective sentences generally refer to personal opinion, emotion or judgment whereas objective refers to factual information. Subjectivity is also a float which lies in the range of [0,1]."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing Stop words**\n",
        "\n",
        "Stop words are used in a language to removed from text data during natural language processing. This helps to reduce the dimensionality of the feature space and focus on the more important words in the text."
      ],
      "metadata": {
        "id": "bIhir0ScTpda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Importing dependancies and removing stopwords.\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Creating argument for stop words.\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "print(stop_words)\n",
        "rest_word=['order','restaurant','taste','ordered','good','food','table','place','one','also']\n",
        "rest_word"
      ],
      "metadata": {
        "id": "ALsSVHk4TuZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "\n",
        "\n",
        "# We will extrapolate the 15 profiles that have made more reviews.\n",
        "\n",
        "# Groupby on the basis of rivewer gives the fequency of the reviews\n",
        "reviewer_list = review_df.groupby('Reviewer').apply(lambda x: x['Reviewer'].count()).reset_index(name='Review_Count')\n",
        "\n",
        " # Sorting the frequency of reviews decending\n",
        "reviewer_list = reviewer_list.sort_values(by = 'Review_Count',ascending=False)\n",
        "\n",
        "# Selecting the top 15 reviewrs\n",
        "top_reviewers = reviewer_list[:15]\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualizing the top 15 reviewers.\n",
        "plt.figure(figsize=(13,5))\n",
        "plt.bar(top_reviewers['Reviewer'], top_reviewers['Review_Count'], color = sns.color_palette(\"hls\", 8))\n",
        "plt.xticks(rotation=75)\n",
        "plt.title('Top 15 reviews',size=28)\n",
        "plt.xlabel(\"Reviewer's Name\",size=15)\n",
        "plt.ylabel('N of reviews',size=15)"
      ],
      "metadata": {
        "id": "QexoiQpMi9FZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of top 15 reviewers based on the number of reviews they have made in a given dataset. Analyzing the reviews made by these top reviewers can help in improving customer satisfaction and loyalty, ultimately leading to increased revenue and growth"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "\n",
        "# Calculate the average of their ratings review.\n",
        "review_ratings=review_df.groupby('Reviewer').apply(lambda x:np.average(x['Rating'])).reset_index(name='Average_Ratings')\n",
        "review_ratings=pd.merge(top_reviewers,review_ratings,how='inner',left_on='Reviewer',right_on='Reviewer')\n",
        "top_reviewers_ratings=review_ratings[:15]\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Average rating of top reviewers.\n",
        "plt.figure(figsize=(15,6))\n",
        "x = top_reviewers_ratings['Average_Ratings']\n",
        "y = top_reviewers_ratings['Reviewer']\n",
        "plt.title(\"Top 15 reviewers with average rating of review\",fontsize=20, weight='bold',color=sns.cubehelix_palette(8, start=.5, rot=90)[-5])\n",
        "plt.ylabel(\"Name\",weight='bold',fontsize=15)\n",
        "plt.xlabel(\"Average Ratings\",weight='bold',fontsize=15)\n",
        "plt.xticks(rotation=90)\n",
        "sns.barplot(x=x, y=y,palette='plasma')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IJ5H4DPjmPZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Barplot helps in understanding the frequency of rating, follower and total reviews with respect to reviewer. Plotting total review, average reviewer rating, and total follower allows to see the correlation between these variables and how they relate to one another for each reviewer. It can also give insight on how reviewers with more followers tend to get more reviews, how their ratings tend to be, etc."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "\n",
        "# Removing Special characters and punctuation from review columns.\n",
        "\n",
        "import re\n",
        "review_df['Review']=review_df['Review'].map(lambda x: re.sub('[,\\.!?]','', x))\n",
        "review_df['Review']=review_df['Review'].map(lambda x: x.lower())\n",
        "review_df['Review']=review_df['Review'].map(lambda x: x.split())\n",
        "review_df['Review']=review_df['Review'].apply(lambda x: [item for item in x if item not in stop_words])\n",
        "review_df['Review']=review_df['Review'].apply(lambda x: [item for item in x if item not in rest_word])"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Word cloud for positive reviews.\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "review_df['Review']=review_df['Review'].astype(str)\n",
        "\n",
        "ps = PorterStemmer()\n",
        "review_df['Review']=review_df['Review'].map(lambda x: ps.stem(x))\n",
        "long_string = ','.join(list(review_df['Review'].values))\n",
        "long_string\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=100, contour_width=3, contour_color='steelblue')\n",
        "wordcloud.generate(long_string)\n",
        "wordcloud.to_image()"
      ],
      "metadata": {
        "id": "FWQmbbCMoCfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Service, taste, time, starters are key to good review."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "\n",
        "# Creating two datasets for positive and negative reviews.\n",
        "\n",
        "review_df['Rating']= pd.to_numeric(review_df['Rating'],errors='coerce')   # The to_numeric() function in pandas is used to convert a pandas object to a numeric type.\n",
        "pos_rev = review_df[review_df.Rating>= 3]\n",
        "neg_rev = review_df[review_df.Rating< 3]\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Negative reviews wordcloud.\n",
        "\n",
        "long_string = ','.join(list(neg_rev['Review'].values))\n",
        "long_string\n",
        "wordcloud = WordCloud(background_color=\"white\", max_words=100, contour_width=3, contour_color='steelblue')\n",
        "wordcloud.generate(long_string)\n",
        "wordcloud.to_image()"
      ],
      "metadata": {
        "id": "VfR40GYio2cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Service , bad chicken , staff behavior, stale food are key reasons for neagtive reviews"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Cleaning**"
      ],
      "metadata": {
        "id": "ss2DhlccYkeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating word embeddings and t-SNE plot. (for positive and negative reviews).\n",
        "\n",
        "from gensim.models import word2vec\n",
        "pos_rev = review_df[review_df.Rating>= 3]\n",
        "neg_rev = review_df[review_df.Rating< 3]"
      ],
      "metadata": {
        "id": "eZY8najEYeR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataframe where the Rating column is greater than or equal to 3. This selects all the positive reviews where as the Rating column is less than 3. This selects all the negative reviews, assuming that the Rating column is a scale from 1 to 5 with 5 being the highest rating."
      ],
      "metadata": {
        "id": "q3kJRd4RZKPF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LDA**"
      ],
      "metadata": {
        "id": "AZvC2xwGcvtQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topic Modeling using LDA\n",
        "\n",
        "LDA is one of the methods to assign topic to texts. If observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's presence is attributable to one of the document's topics."
      ],
      "metadata": {
        "id": "GOba2hxjc6aC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from gensim.utils import simple_preprocess"
      ],
      "metadata": {
        "id": "WbXYzQhCdCDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Plotting the top 10 most occuring words. Topic modeling is a process to automatically identify topics present in a text object and to assign text corpus to one category of topic."
      ],
      "metadata": {
        "id": "YU22Nlt-dIZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume that documents is a list of strings representing text documents\n",
        "\n",
        "# Tokenize the documents\n",
        "tokenized_docs = [simple_preprocess(doc) for doc in review_df['Review']]\n",
        "\n",
        "# Create a dictionary from the tokenized documents\n",
        "dictionary = corpora.Dictionary(tokenized_docs)\n",
        "\n",
        "# Convert the tokenized documents to a bag-of-words corpus\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "\n",
        "# Train an LDA model on the bag-of-words corpus\n",
        "num_topics = 10  # The number of topics to extract\n",
        "lda_model = LdaModel(bow_corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
        "\n",
        "# Print the topics and their top 10 terms\n",
        "for topic in lda_model.show_topics(num_topics=num_topics, num_words=10, formatted=False):\n",
        "    print('Topic {}: {}'.format(topic[0], ', '.join([term[0] for term in topic[1]])))"
      ],
      "metadata": {
        "id": "f4xf99ifdOWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyLDAvis"
      ],
      "metadata": {
        "id": "_beVkakTdppj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import pyLDAvis.gensim\n",
        "import pyLDAvis.sklearn\n",
        "pyLDAvis.enable_notebook()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7rD0LJMydzCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lda_visualization = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary, mds='tsne')\n",
        "pyLDAvis.display(lda_visualization)"
      ],
      "metadata": {
        "id": "Oo9ryvUHgG5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The topics and topic terms can be visualised to help assess how interpretable the topic model is."
      ],
      "metadata": {
        "id": "yFNk3jmMhSlG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentiment Analysis**"
      ],
      "metadata": {
        "id": "Sz1BVtN-hYUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import plotly.express as px\n"
      ],
      "metadata": {
        "id": "h2nRJ46phe5s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to get the subjectivity\n",
        "def subjectivity(text):\n",
        "    return TextBlob(text).sentiment.subjectivity"
      ],
      "metadata": {
        "id": "NNiFCIymhk5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to get the subjectivity\n",
        "def subjectivity(text):\n",
        "    return TextBlob(text).sentiment.subjectivity"
      ],
      "metadata": {
        "id": "rhAMbzePhtJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying subjectivity and the polarity function to the respective columns\n",
        "review_df['Subjectivity'] = review_df['Review'].apply(subjectivity)\n",
        "review_df['Polarity'] = review_df['Review'].apply(polarity)"
      ],
      "metadata": {
        "id": "fsS_dTlLhzco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for created columns\n",
        "review_df['Polarity']"
      ],
      "metadata": {
        "id": "IK3qXN3Wh47k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for created columns\n",
        "review_df['Subjectivity']"
      ],
      "metadata": {
        "id": "QprjT9xYiGKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to compute the negative, neutral and positive analysis\n",
        "def getAnalysis(score):\n",
        "    if score <0:\n",
        "        return 'Negative'\n",
        "    elif score == 0:\n",
        "        return 'Neutral'\n",
        "    else:\n",
        "        return 'Positive'"
      ],
      "metadata": {
        "id": "IIU3kdt_iNe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "If the score is less than 0, the function returns the string 'Negative'. If the score is equal to 0, the function returns the string 'Neutral'. If the score is greater than 0, the function returns the string 'Positive'."
      ],
      "metadata": {
        "id": "LBL-DXihiVkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply get analysis function to separate the sentiments from the column\n",
        "review_df['Analysis'] = review_df['Polarity'].apply(getAnalysis)"
      ],
      "metadata": {
        "id": "drQOUhA7icRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the polarity and subjectivity\n",
        "fig = px.scatter(review_df,\n",
        "                 x='Polarity',\n",
        "                 y='Subjectivity',\n",
        "                 color = 'Analysis',\n",
        "                 size='Subjectivity')"
      ],
      "metadata": {
        "id": "Ds3bgQAUiiHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a vertical line at x=0 for Netural Reviews\n",
        "fig.update_layout(title='Sentiment Analysis',\n",
        "                  shapes=[dict(type= 'line',\n",
        "                               yref= 'paper', y0= 0, y1= 1,\n",
        "                               xref= 'x', x0= 0, x1= 0)])\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "-_PchQL_ioBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting plot can provide several insights into the sentiment analysis results. Firstly, the histogram bars on the left side of the plot (negative polarity) indicate that a significant number of reviews expressed negative sentiments. Similarly, the histogram bars on the right side of the plot (positive polarity) indicate that a significant number of reviews expressed positive sentiments.\n",
        "\n",
        "Overall, this plot can provide a quick and easy way to visualize the sentiment polarity distribution of the reviews, which can help in understanding the overall sentiment of the customers towards the restaurants."
      ],
      "metadata": {
        "id": "ovA1MH_GiwAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clustering**"
      ],
      "metadata": {
        "id": "qbLYPbnRi4le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning);"
      ],
      "metadata": {
        "id": "XPRTE8Ri8vAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**K-means Clustering**"
      ],
      "metadata": {
        "id": "qqVt7uQADKwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from yellowbrick.cluster import KElbowVisualizer"
      ],
      "metadata": {
        "id": "gTkUdiZJDKR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Create a corpus of words from the negative reviews in the neg_rev DataFrame."
      ],
      "metadata": {
        "id": "IKJrqNifwWLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot for negative reviews.\n",
        "def build_corpus(data):\n",
        "    \"Creates a list of lists containing words from each sentence\"\n",
        "    corpus = []\n",
        "    for col in ['Review']:\n",
        "        for sentence in data[col].iteritems():\n",
        "            word_list = sentence[1].split(\" \")\n",
        "            corpus.append(word_list)\n",
        "\n",
        "    return corpus\n",
        "\n",
        "# Display the first two elements of the corpus list\n",
        "corpus = build_corpus(neg_rev)\n",
        "corpus[0:2]"
      ],
      "metadata": {
        "id": "jJMeG6OwwjsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot for postive reviews\n",
        "def build_corpus(data):\n",
        "    \"Creates a list of lists containing words from each sentence\"\n",
        "    corpus = []\n",
        "    for col in ['Review']:\n",
        "        for sentence in data[col].iteritems():\n",
        "            word_list = sentence[1].split(\" \")\n",
        "            corpus.append(word_list)\n",
        "\n",
        "    return corpus\n",
        "\n",
        "# Display the first two elements of the corpus list\n",
        "corpus = build_corpus(pos_rev)\n",
        "corpus[0:2]"
      ],
      "metadata": {
        "id": "w0q84xxLZZjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The cost of a restaurant is positively correlated with the rating it receives.\n",
        "\n",
        "* Restaurants that are reviewed by reviewers with more followers will have a higher rating.\n",
        "\n",
        "* Restaurants that offer a wider variety of cuisines will have a higher rating."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The cost of a restaurant is positively correlated with the rating it receives."
      ],
      "metadata": {
        "id": "SWkIa81tvzog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Null hypothesis: There is no relationship between the cost of restaurant and the rating it receives. (H0: 1 = 0)\n",
        "\n",
        "* Alternative hypothesis: There is a positive relationship between the cost of a restaurant and the rating it receives. (H1: 1 > 0)\n",
        "\n",
        "* Test : Simple Linear Regression Analysis"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "# Perform Statistical Test to obtain P-Value\n",
        "import statsmodels.formula.api as smf\n",
        "\n",
        "\n",
        "# fit the linear model\n",
        "model = smf.ols(formula='Rating ~ Cost', data= merged).fit()\n",
        "\n",
        "# Check p-value of coefficient\n",
        "p_value = model.pvalues[1]\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject Null Hypothesis - There is no relationship between the cost of\\\n",
        " restaurant and the rating it receives.\")\n",
        "else:\n",
        "    print(\"Fail to reject Null Hypothesis - There is a positive relationship \\\n",
        " between the cost of a restaurant and the rating it receives.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Linear regression test for checking the relationship between the cost of a restaurant and its rating"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose this test because it is a common and straightforward method for testing the relationship between two continuous variables. This would involve fitting a linear model with the rating as the dependent variable and the cost as the independent variable. The p-value of the coefficient for the cost variable can then be used to determine if there is a statistically significant relationship between the two variables."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restaurants that are reviewed by reviewers with more followers will have a higher rating."
      ],
      "metadata": {
        "id": "enYdfjfzJrgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Null hypothesis: The number of followers a reviewer has has no effect on the rating of a restaurant. (H0: 1 = 0)\n",
        "\n",
        "* Alternative hypothesis: Alternative Hypothesis: The number of followers a reviewer has has a positive effect on the rating of a restaurant. (H1: 1 > 0)\n",
        "\n",
        "* Test : Simple Linear Regression test"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the second hypothesis, I have used Simple Linear Regression Test."
      ],
      "metadata": {
        "id": "hnL-zjNbbrpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I choose this test because it is a straightforward method for testing the relationship between two continuous variables. It assumes that there is a linear relationship between the independent variable (Reviewer_Followers) and the dependent variable (Rating) and it allows us to estimate the strength and direction of that relationship. It also allows us to test the null hypothesis that there is no relationship between the two variables by testing the p-value of the coefficient of the independent variable."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restaurants that offer a wider variety of cuisines will have a higher rating."
      ],
      "metadata": {
        "id": "ct3vhoYKctjj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Null hypothesis: The variety of cuisines offered by a restaurant has no effect on its rating. (H0: 3 = 0)\n",
        "\n",
        "* Alternative hypothesis: The variety of cuisines offered by a restaurant has a positive effect on its rating. (H1: 3 > 0)\n",
        "\n",
        "* Test : Chi-Squared Test"
      ],
      "metadata": {
        "id": "HcXildTXdb5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.crosstab(merged['Cuisines'], merged['Rating'])[:1]"
      ],
      "metadata": {
        "id": "kzUsIx2ueoi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# create a contingency table\n",
        "ct = pd.crosstab(merged['Cuisines'], merged['Rating'])\n",
        "\n",
        "# perform chi-squared test\n",
        "chi2, p, dof, expected = chi2_contingency(ct)\n",
        "\n",
        "# Check p-value\n",
        "if p < 0.05:\n",
        "    print(\"Reject Null Hypothesis\")\n",
        "else:\n",
        "    print(\"Fail to reject Null Hypothesis\")\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the third hypothesis, I have used chi-squared test for independence to test the relationship between the variety of cuisines offered by a restaurant and its rating."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I choose this test because it is suitable for comparing the relationship between two categorical variables. This would involve creating a contingency table with the number of restaurants that offer each cuisine as the rows and the rating of the restaurant as the columns."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Treating Duplicates**"
      ],
      "metadata": {
        "id": "eIeq_FGygNQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Since all the duplicated data has NaN values, hence dropping the entire values as it will not help and will create unnecessary noise."
      ],
      "metadata": {
        "id": "JU0FE8wWgfur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "#deleting duplicate value from review dataset\n",
        "review = review.drop_duplicates()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#final check after dropping duplicates\n",
        "print(f\"Anymore duplicate left ? {review.duplicated().value_counts()}, unique values with {len(review[review.duplicated()])} duplication\")"
      ],
      "metadata": {
        "id": "cm3g7RZHg-qk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Treating Missing Values**"
      ],
      "metadata": {
        "id": "Y6NVKavDjcZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restaurant Dataset"
      ],
      "metadata": {
        "id": "_kk09Jf8jqUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "restaurant.isnull().sum()"
      ],
      "metadata": {
        "id": "FKn5E7Aojx67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the null value in timing\n",
        "hotel[hotel['Timings'].isnull()]"
      ],
      "metadata": {
        "id": "LOn_eooSrAsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filling null value in timings column\n",
        "hotel.Timings.fillna(hotel.Timings.mode()[0], inplace = True)"
      ],
      "metadata": {
        "id": "4y1I2algrI4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking null values in Collections\n",
        "missing_percentage = ((hotel['Collections'].isnull().sum())/(len(hotel['Collections'])))*100\n",
        "print(f'Percentage of missing value in Collections is {round(missing_percentage, 2)}%')"
      ],
      "metadata": {
        "id": "tbG4Y86krK8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping collection column since has more than 50% of null values\n",
        "hotel.drop('Collections', axis = 1, inplace = True)"
      ],
      "metadata": {
        "id": "kpiCcxNirO8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Review Dataset**"
      ],
      "metadata": {
        "id": "49e_4gKYraBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#review missing value\n",
        "review.isnull().sum()"
      ],
      "metadata": {
        "id": "pCPnDpWSrifR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking null reviewer\n",
        "review[review['Reviewer'].isnull()"
      ],
      "metadata": {
        "id": "0ejuTmberoKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping null values in reviewer and Reviewer_Total_Review column as all values are null for those column\n",
        "review = review.dropna(subset=['Reviewer','Reviewer_Total_Review'])"
      ],
      "metadata": {
        "id": "7lKBYzZsrvqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#again checking the remaining values\n",
        "null_counts = [(x, a) for x, a in review.isnull().sum().items() if a > 0]\n",
        "\n",
        "# Print the columns with null values\n",
        "null_counts"
      ],
      "metadata": {
        "id": "iYau_aeirwoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filling null values in review and reviewer follower column\n",
        "review = review.fillna({\"Review\": \"No Review\", \"Reviewer_Followers\": 0})"
      ],
      "metadata": {
        "id": "P3vDTW1pr2JJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merging both dataset\n",
        "merged = hotel.merge(review, on = 'Restaurant')\n",
        "merged.shape"
      ],
      "metadata": {
        "id": "W7jV0NAusC1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I started treating missing values by first removing the duplicate data where all other values were NaN or null values except had restaurant name, so instead of replacing each null value I removed it as it was only 36 duplicate data which had no unique identity.\n",
        "\n",
        "Dataset that contains details about hotel, had 1 null value in timing feature and more than 50% null value in collection feature. In order to treat with those I first replaced the null value for timing with mode since there was only one null and mode is robust to outliers plus that hotel name was one unique feature which had all other feature except timing and collection so it was better to preserve that data. Since there was more than 50% null values in collection feature, I removed the entire column because columns with a high percentage of null values are likely to have a lot of missing data, which can make it difficult to accurately analyze or make predictions based on the data.\n",
        "\n",
        "In the dataset tha has details of reviewer had Reviewer - 2, Review - 9, Rating - 2, Metadata - 2, Time - 2, Reviewer_Total_Review- 3, Reviewer_Followers - 1581, Review_Year - 2, Review_Month - 2, Review_Hour - 2. On analysing I found that feature like reviewer and reviewer total review had all null values, therefore I removed those two columns which made null values in other feature to zero except in review and reviewer followers columns. Since review was textual data, I changed those 7 null values to 'no review' and reviewer followers to 0 as follower is the meta data for reviewer and it can be 0.\n",
        "\n",
        "And thus all the null values were treated, at the end I then again merged both the dataset hotel and review dataset."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Detecting Anamoly**"
      ],
      "metadata": {
        "id": "JQu88m1m04fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "#Anamoly detection\n",
        "from sklearn.ensemble import IsolationForest\n",
        "#checking for normal distribution\n",
        "print(\"Skewness - Cost: %f\" % merged['Cost'].skew())\n",
        "print(\"Kurtosis - Cost: %f\" % merged['Cost'].kurt())\n",
        "print(\"Skewness - Reviewer_Followers: %f\" % merged['Reviewer_Followers'].skew())\n",
        "print(\"Kurtosis - Reviewer_Followers: %f\" % merged['Reviewer_Followers'].kurt())\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting graph for cost\n",
        "plt.scatter(range(merged.shape[0]), np.sort(merged['Cost'].values))\n",
        "plt.xlabel('index')\n",
        "plt.ylabel('Cost')\n",
        "plt.title(\"Cost distribution\")\n",
        "sns.despine()\n"
      ],
      "metadata": {
        "id": "u2auhEpJsdhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#distribution ofost\n",
        "sns.distplot(merge cd['Cost'])\n",
        "plt.title(\"Distribution of Cost\")\n",
        "sns.despine()"
      ],
      "metadata": {
        "id": "VRGwOde9slPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot for reviewer follower\n",
        "plt.scatter(range(merged.shape[0]), np.sort(merged['Reviewer_Followers'].values))\n",
        "plt.xlabel('index')\n",
        "plt.ylabel('Reviewer_Followers')\n",
        "plt.title(\"Reviewer_Followers distribution\")\n",
        "sns.despine()"
      ],
      "metadata": {
        "id": "QAwwPvbxsq9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot for reviewer follower\n",
        "plt.scatter(range(merged.shape[0]), np.sort(merged['Reviewer_Followers'].values))\n",
        "plt.xlabel('index')\n",
        "plt.ylabel('Reviewer_Followers')\n",
        "plt.title(\"Reviewer_Followers distribution\")\n",
        "sns.despine()"
      ],
      "metadata": {
        "id": "Q378bc3KtEvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#isolation forest for anamoly detection on cost\n",
        "isolation_forest = IsolationForest(n_estimators=100, contamination=0.01)\n",
        "isolation_forest.fit(merged['Cost'].values.reshape(-1, 1))\n",
        "merged['anomaly_score_univariate_Cost'] = isolation_forest.decision_function(merged['Cost'].values.reshape(-1, 1))\n",
        "merged['outlier_univariate_Cost'] = isolation_forest.predict(merged['Cost'].values.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "iKyCrFOQtXMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#chart to visualize outliers\n",
        "xx = np.linspace(merged['Cost'].min(), merged['Cost'].max(), len(merged)).reshape(-1,1)\n",
        "anomaly_score = isolation_forest.decision_function(xx)\n",
        "outlier = isolation_forest.predict(xx)\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(xx, anomaly_score, label='anomaly score')\n",
        "plt.fill_between(xx.T[0], np.min(anomaly_score), np.max(anomaly_score),\n",
        "where=outlier==-1, color='r',\n",
        "alpha=.4, label='outlier region')\n",
        "plt.legend()\n",
        "plt.ylabel('anomaly score')\n",
        "plt.xlabel('Cost')\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "SgjWHEN5teQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#chart to visualize outliers\n",
        "xx = np.linspace(merged['Cost'].min(), merged['Cost'].max(), len(merged)).reshape(-1,1)\n",
        "anomaly_score = isolation_forest.decision_function(xx)\n",
        "outlier = isolation_forest.predict(xx)\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(xx, anomaly_score, label='anomaly score')\n",
        "plt.fill_between(xx.T[0], np.min(anomaly_score), np.max(anomaly_score),\n",
        "where=outlier==-1, color='r',\n",
        "alpha=.4, label='outlier region')\n",
        "plt.legend()\n",
        "plt.ylabel('anomaly score')\n",
        "plt.xlabel('Cost')\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "qx-UMd1ztqpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#chat to visualize outliers in reviwer follower column\n",
        "xx = np.linspace(merged['Reviewer_Followers'].min(), merged['Reviewer_Followers'].max(), len(merged)).reshape(-1,1)\n",
        "anomaly_score = isolation_forest.decision_function(xx)\n",
        "outlier = isolation_forest.predict(xx)\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(xx, anomaly_score, label='anomaly score')\n",
        "plt.fill_between(xx.T[0], np.min(anomaly_score), np.max(anomaly_score),\n",
        "where=outlier==-1, color='r',\n",
        "alpha=.4, label='outlier region')\n",
        "plt.legend()\n",
        "plt.ylabel('anomaly score')\n",
        "plt.xlabel('Reviewer_Followers')\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "tdR-BFKKtr5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Treating Outlier**"
      ],
      "metadata": {
        "id": "IjXWPj48kCri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For Skew Symmetric features defining upper and lower boundry\n",
        "#Outer Fence\n",
        "def outlier_treatment_skew(df,feature):\n",
        "  IQR= df[feature].quantile(0.75)- df[feature].quantile(0.25)\n",
        "  lower_bridge =df[feature].quantile(0.25)- 1.5*IQR\n",
        "  upper_bridge =df[feature].quantile(0.75)+ 1.5*IQR\n",
        "  # print(f'upper : {upper_bridge} lower : {lower_bridge}')\n",
        "  return upper_bridge,lower_bridge"
      ],
      "metadata": {
        "id": "E9Wl5SjHmOMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restricting the data to lower and upper boundary for cost in hotel dataset\n",
        "#lower limit capping\n",
        "hotel.loc[hotel['Cost']<= outlier_treatment_skew(df=hotel,\n",
        "  feature='Cost')[1], 'Cost']=outlier_treatment_skew(df=hotel,feature='Cost')[1]\n",
        "\n",
        "#upper limit capping\n",
        "hotel.loc[hotel['Cost']>= outlier_treatment_skew(df=hotel,\n",
        "  feature='Cost')[0], 'Cost']=outlier_treatment_skew(df=hotel,feature='Cost')[0]"
      ],
      "metadata": {
        "id": "hS8XY3hxmVEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restricting the data to lower and upper boundary for Reviewer followers in review dataset\n",
        "#lower limit capping\n",
        "review.loc[review['Reviewer_Followers']<= outlier_treatment_skew(df=review,\n",
        "  feature='Reviewer_Followers')[1], 'Reviewer_Followers']=outlier_treatment_skew(\n",
        "      df=review,feature='Reviewer_Followers')[1]\n",
        "\n",
        "#upper limit capping\n",
        "review.loc[review['Reviewer_Followers']>= outlier_treatment_skew(df=review,\n",
        "  feature='Reviewer_Followers')[0], 'Reviewer_Followers']=outlier_treatment_skew(\n",
        "      df=review,feature='Reviewer_Followers')[0]"
      ],
      "metadata": {
        "id": "BdQ5PfTAmWzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping the columns created while outliers treatment\n",
        "merged.drop(columns =['anomaly_score_univariate_Cost','outlier_univariate_Cost',\n",
        "  'anomaly_score_univariate_follower','outlier_univariate_follower'], inplace = True)"
      ],
      "metadata": {
        "id": "rtiKKfh6mhAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since cost and reviewer follower feature or column show positive skewed distribution and using isolation forest found they have outliers, hence using the capping technique instead of removing the outliers, capped outliers with the highest and lowest limit using IQR method."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "# Encode your categorical columns\n",
        "\n",
        "#categorial encoding using pd.getdummies\n",
        "#new df with important categories\n",
        "cluster_dummy = hotel[['Restaurant','Cuisines']]\n",
        "#spliting cuisines as they are separted with comma and converting into list\n",
        "cluster_dummy['Cuisines'] = cluster_dummy['Cuisines'].str.split(',')\n",
        "#using explode converting list to unique individual items\n",
        "cluster_dummy = cluster_dummy.explode('Cuisines')\n",
        "#removing extra trailing space from cuisines after exploded\n",
        "cluster_dummy['Cuisines'] = cluster_dummy['Cuisines'].apply(lambda x: x.strip())\n",
        "#using get dummies to get dummies for cuisines\n",
        "cluster_dummy = pd.get_dummies(cluster_dummy, columns=[\"Cuisines\"], prefix=[\"Cuisines\"])\n",
        "\n",
        "#checking if the values are correct\n",
        "# cluster_dummy.loc[:, cluster_dummy.columns.str.startswith('Cuisines_')].eq(1)[:5].T\n",
        "cluster_dummy.loc[:, cluster_dummy.columns.str.startswith('Cuisines_')].idxmax(1)[:6]\n",
        "\n",
        "#replacing cuisines_ from columns name - for better understanding run seperatly\n",
        "\n",
        "# cluster_dummy.columns = cluster_dummy.columns.str.replace(\" \",\"\")\n",
        "cluster_dummy.columns = cluster_dummy.columns.str.replace(\"Cuisines_\",\"\")\n",
        "# cluster_dummy = cluster_dummy.groupby(cluster_dummy.columns, axis=1).sum()\n",
        "\n",
        "#grouping each restaurant as explode created unnecessary rows\n",
        "cluster_dummy = cluster_dummy.groupby(\"Restaurant\").sum().reset_index()"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping the columns created while outliers treatment\n",
        "merged.drop(columns =['anomaly_score_univariate_Cost','outlier_univariate_Cost',\n",
        "  'anomaly_score_univariate_follower','outlier_univariate_follower'], inplace = True)"
      ],
      "metadata": {
        "id": "rZUmQvfgmxft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adding average rating - will remove 5 unrated restaurant from 105 restaurant\n",
        "avg_hotel_rating.rename(columns = {'Rating':'Average_Rating'}, inplace =True)\n",
        "hotel = hotel.merge(avg_hotel_rating[['Average_Rating','Restaurant']], on = 'Restaurant')\n",
        "hotel.head(1)"
      ],
      "metadata": {
        "id": "44HuykzRmzAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adding cost column to the new dataset\n",
        "cluster_dummy = hotel[['Restaurant','Cost','Average_Rating','Total_Cuisine_Count'\n",
        "                      ]].merge(cluster_dummy, on = 'Restaurant')"
      ],
      "metadata": {
        "id": "a1fUuG07m9y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xEqBK2zAnDBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely."
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1  Clustering"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KMeans Clustering"
      ],
      "metadata": {
        "id": "-fS7_V7H0-Fn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "K-Means Clustering is an Unsupervised Learning algorithm.The algorithm takes the unlabeled dataset as input, divides the dataset into k-number of clusters, and repeats the process until it does not find the best clusters. The value of k should be predetermined in this algorithm.\n",
        "\n",
        "It is a centroid-based algorithm, where each cluster is associated with a centroid. The main aim of this algorithm is to minimize the sum of distances between the data point and their corresponding clusters.\n",
        "\n",
        "The k-means clustering algorithm mainly performs two tasks:\n",
        "\n",
        "Determines the best value for K center points or centroids by an iterative process.\n",
        "\n",
        "Assigns each data point to its closest k-center. Those data points which are near to the particular k-center, create a cluster.\n",
        "\n",
        "**ELBOW METHOD**\n",
        "\n",
        "This method uses the concept of WCSS value. WCSS stands for Within Cluster Sum of Squares, which defines the total variations within a cluster.\n",
        "\n",
        "**SILHOUETTE METHOD**\n",
        "\n",
        "The silhouette coefficient or silhouette score kmeans is a measure of how similar a data point is within-cluster (cohesion) compared to other clusters (separation)."
      ],
      "metadata": {
        "id": "M_ETBHp51G_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from yellowbrick.cluster import KElbowVisualizer"
      ],
      "metadata": {
        "id": "pLkE_jhqvcWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of inertia scores for different numbers of clusters\n",
        "scores = [KMeans(n_clusters=i+2, random_state=11).fit(df_cluster.drop('Name',axis=1)).inertia_\n",
        "          for i in range(8)]\n",
        "\n",
        "# Create a line plot of inertia scores versus number of clusters\n",
        "plt.figure(figsize=(7,7))\n",
        "sns.lineplot(x=np.arange(2, 10), y=scores)\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Inertia of k-Means versus number of clusters')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pA4hpK_bvic4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "#importing kmeans\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "#Within Cluster Sum of Squared Errors(WCSS) for different values of k\n",
        "wcss=[]\n",
        "for i in range(1,11):\n",
        "    km=KMeans(n_clusters=i,random_state = 20)\n",
        "    km.fit(df_pca)\n",
        "    wcss.append(km.inertia_)\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#elbow curve\n",
        "plt.plot(range(1,11),wcss)\n",
        "plt.plot(range(1,11),wcss, linewidth=2, color=\"red\", marker =\"o\")\n",
        "plt.xlabel(\"K Value\", size = 20, color = 'purple')\n",
        "plt.xticks(np.arange(1,11,1))\n",
        "plt.ylabel(\"WCSS\", size = 20, color = 'green')\n",
        "plt.title('Elbow Curve', size = 20, color = 'blue')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1mTaqPGm65LJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#silhouette score\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics import silhouette_samples\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "# candidates for the number of cluster\n",
        "parameters = list(range(2,10))\n",
        "#parameters\n",
        "parameter_grid = ParameterGrid({'n_clusters': parameters})\n",
        "best_score = -1\n",
        "#visualizing Silhouette Score for individual clusters and the clusters made\n",
        "for n_clusters in parameters:\n",
        "    # Create a subplot with 1 row and 2 columns\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
        "    fig.set_size_inches(18, 7)\n",
        "\n",
        "    # 1st subplot is the silhouette plot\n",
        "    # silhouette coefficient can range from -1, 1 but in this example all\n",
        "    # lie within [-0.1, 1]\n",
        "    ax1.set_xlim([-0.1, 1])\n",
        "    # (n_clusters+1)*10 is for inserting blank space between silhouette\n",
        "    # plots of individual clusters, to demarcate them clearly.\n",
        "    ax1.set_ylim([0, len(df_pca) + (n_clusters + 1) * 10])\n",
        "\n",
        "    # Initialize the clusterer with n_clusters value and a random generator\n",
        "    # seed of 10 for reproducibility.\n",
        "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
        "    cluster_labels = clusterer.fit_predict(df_pca)\n",
        "\n",
        "    # silhouette_score gives the average value for all the samples.\n",
        "    # This gives a perspective into the density and separation of the formed\n",
        "    # clusters\n",
        "    silhouette_avg = silhouette_score(df_pca, cluster_labels)\n",
        "    print(\"For n_clusters =\", n_clusters,\n",
        "          \"average silhouette_score is :\", silhouette_avg)\n",
        "\n",
        "    # Compute the silhouette scores for each sample\n",
        "    sample_silhouette_values = silhouette_samples(df_pca, cluster_labels)\n",
        "\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        # Aggregate the silhouette scores for samples belonging to\n",
        "        # cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = \\\n",
        "            sample_silhouette_values[cluster_labels == i]\n",
        "\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
        "                          0, ith_cluster_silhouette_values,\n",
        "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
        "\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
        "\n",
        "    ax1.set_title(\"silhouette plot for the various clusters.\")\n",
        "    ax1.set_xlabel(\"silhouette coefficient values\")\n",
        "    ax1.set_ylabel(\"Cluster label\")\n",
        "\n",
        "    # vertical line for average silhouette score of all the values\n",
        "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
        "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "\n",
        "    # 2nd Plot showing the actual clusters formed\n",
        "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
        "    ax2.scatter(df_pca[:, 0], df_pca[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
        "                c=colors, edgecolor='k')\n",
        "\n",
        "    # Labeling the clusters\n",
        "    centers = clusterer.cluster_centers_\n",
        "    # Draw white circles at cluster centers\n",
        "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
        "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
        "    #marker='$%d$' % i will give numer in cluster in 2 plot\n",
        "    for i, c in enumerate(centers):\n",
        "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
        "                    s=50, edgecolor='k')\n",
        "\n",
        "    ax2.set_title(\"visualization of the clustered data.\")\n",
        "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
        "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
        "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
        "                  \"with n_clusters = %d\" % n_clusters),\n",
        "                 fontsize=14, fontweight='bold')"
      ],
      "metadata": {
        "id": "RQ4fWwNN7Zrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vizualizing the clusters and the datapoints in each clusters\n",
        "plt.figure(figsize = (10,6), dpi = 120)\n",
        "\n",
        "kmeans= KMeans(n_clusters = 5, init= 'k-means++', random_state = 42)\n",
        "kmeans.fit(df_pca)\n",
        "\n",
        "#predict the labels of clusters.\n",
        "label = kmeans.fit_predict(df_pca)\n",
        "#Getting unique labels\n",
        "unique_labels = np.unique(label)\n",
        "\n",
        "#plotting the results:\n",
        "for i in unique_labels:\n",
        "    plt.scatter(df_pca[label == i , 0] , df_pca[label == i , 1] , label = i)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eyYkToiC7mrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#making df for pca\n",
        "kmeans_pca_df = pd.DataFrame(df_pca,columns=['PC1','PC2','PC3'],index=scaled_df.index)\n",
        "kmeans_pca_df[\"label\"] = label\n",
        "kmeans_pca_df.sample(2)"
      ],
      "metadata": {
        "id": "qTnpE0d27zIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#joining the cluster labels to names dataframe\n",
        "cluster_dummy.set_index(['Restaurant'],inplace=True)\n",
        "cluster_dummy = cluster_dummy.join(kmeans_pca_df['label'])\n",
        "cluster_dummy.sample(2)"
      ],
      "metadata": {
        "id": "RTaYA6zY76Dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#changing back cost value to original from log1p done during transformation\n",
        "cluster_dummy['Cost'] = np.expm1(cluster_dummy['Cost'])\n",
        "clustering_result = hotel[['Restaurant','Cuisines']].merge(clustering_result[['Restaurant','Cost',\n",
        "cluster_dummy.sample(2)"
      ],
      "metadata": {
        "id": "3KdHspiw8AKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating df to store cluster data\n",
        "clustering_result = cluster_dummy.copy().reset_index()\n",
        "clustering_result = hotel[['Restaurant','Cuisines']].merge(clustering_result[['Restaurant','Cost',\n",
        "                  'Average_Rating',\t'Total_Cuisine_Count','label']], on = 'Restaurant')\n",
        "clustering_result.head()"
      ],
      "metadata": {
        "id": "C6Gf_D029NR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting content in each cluster\n",
        "cluster_count = cluster_dummy['label'].value_counts().reset_index().rename(\n",
        "    columns={'index':'label','label':'Total_Restaurant'}).sort_values(by='Total_Restaurant')\n",
        "cluster_count"
      ],
      "metadata": {
        "id": "I6q2R2Mr9VwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating new df for checkign cuising in each cluster\n",
        "new_cluster_df = clustering_result.copy()\n",
        "new_cluster_df['Cuisines'] = new_cluster_df['Cuisines'].str.split(',')\n",
        "new_cluster_df = new_cluster_df.explode('Cuisines')\n",
        "#removing extra trailing space from cuisines after exploded\n",
        "new_cluster_df['Cuisines'] = new_cluster_df['Cuisines'].apply(lambda x: x.strip())\n",
        "new_cluster_df.sample(5)"
      ],
      "metadata": {
        "id": "vZjA8csj9gHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#printing cuisine list for each cluster\n",
        "for cluster in new_cluster_df['label'].unique().tolist():\n",
        "  print('Cuisine List for Cluster :', cluster,'\\n')\n",
        "  print(new_cluster_df[new_cluster_df[\"label\"]== cluster]['Cuisines'].unique(),'\\n')\n",
        "  print('='*120)"
      ],
      "metadata": {
        "id": "lt_QCEwC9mAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File\n",
        "# import joblib\n",
        "\n",
        "# Save the model to a file\n",
        "# joblib.dump(model, 'model.joblib')"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "# Load the File and predict unseen data.\n",
        "# Load the File and predict unseen data.\n",
        "# Load the model from the file\n",
        "# model = joblib.load('model.joblib')\n",
        "\n",
        "# Use the model to make predictions\n",
        "# predictions = model.predict(data)"
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Flask code**"
      ],
      "metadata": {
        "id": "mOwx8BDvye6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Flask code\n",
        "# from flask import Flask, request, jsonify\n",
        "# import pickle\n",
        "\n",
        "# app = Flask(__name__)\n",
        "\n",
        "# # Load the pre-trained model\n",
        "# model = pickle.load(open('sentiment_model.pkl', 'rb'))\n",
        "\n",
        "# @app.route('/predict', methods=['POST'])\n",
        "# def predict():\n",
        "#     # Get the text from the request\n",
        "#     text = request.json['text']\n",
        "\n",
        "#     # Use the model to predict the sentiment\n",
        "#     prediction = model.predict([text])[0]\n",
        "\n",
        "#     # Return the prediction as a JSON response\n",
        "#     return jsonify({'sentiment': prediction})\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     app.run(debug=True)\n"
      ],
      "metadata": {
        "id": "tZntshdXyt3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Clustering and sentiment analysis were performed on a dataset of customer reviews for the food delivery service Zomato. The purpose of this analysis was to understand the customer's experience and gain insights about their feedback.\n",
        "\n",
        "The clustering technique was applied to group customers based on their review text, and it was found that the customers were grouped into two clusters: positive and negative. This provided a general understanding of customer satisfaction levels, with the positive cluster indicating the highest level of satisfaction and the negative cluster indicating the lowest level of satisfaction.\n",
        "\n",
        "Sentiment analysis was then applied to classify the review text as positive or negative. This provided a more detailed understanding of customer feedback and helped to identify specific areas where the service could be improved.\n",
        "\n",
        "Overall, this analysis provided valuable insights into the customer's experience with Zomato, and it could be used to guide future business decisions and improve the service. Additionally, by combining clustering and sentiment analysis techniques, a more comprehensive understanding of customer feedback was achieved.\n",
        "\n",
        "Other important discoveries during analysis are -\n",
        "\n",
        "AB's - Absolute Barbecues, show maximum engagement and retention as it has maximum number of rating on average and Hotel Zara Hi-Fi show lowest engagement as has lowest average rating.\n",
        "\n",
        "Price point for high rated hotel AB's= Absolute Barbecues is 1500 and price point for low rated restaurant Hotel Zara Hi-Fi is 400.\n",
        "\n",
        "North Indian food followed by chinese are best or indeemand food as sold by most of the restaurants.\n",
        "\n",
        "Great Buffets is the most frequently used tags and other tags like great, best, north, Hyderabad is also used in large quantity.\n",
        "\n",
        "Satwinder singh is the most popular critic who has maximum number of follower and on an average he give 3.5 rating.\n",
        "\n",
        "restaurant Collage - Hyatt Hyderabad Gachibowli is most expensive restaurant in the locality which has a price of 2800 for order and has 3.5 average rating. Hotels like Amul and Mohammedia Shawarma are least expensive with price of 150 and has 3.9 average rating.\n",
        "\n",
        "Some recommendation based on the analysis \n",
        "\n",
        "Based on negative reviews like some focused on issues with delivery time or food quality, the company should prioritize addressing these issues to imporve customer satisfaction.\n",
        "\n",
        "Based on the clustering, or user interaction customer should be given recommendations.\n",
        "\n",
        "Also use the clustering results to target specific customer segments and tailor marketing and promotional efforts accordingly.\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}